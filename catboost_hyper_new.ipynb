{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621083489967
        }
      },
      "outputs": [],
      "source": [
        "! pip install catboost\n",
        "! pip install --upgrade scikit-learn\n",
        "! pip install openpyxl\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621083746770
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def ml_pipeline (fil):\n",
        "  # importing df and basic cleaning\n",
        "  df_ml = pd.read_csv(fil)\n",
        "  df_ml[\"Forecasttidspunkt\"] = pd.to_datetime(df_ml[\"Forecasttidspunkt\"])\n",
        "  df_ml[\"periode_regnskabstal\"] = pd.to_datetime(df_ml[\"periode_regnskabstal\"])\n",
        "  df_ml.set_index(\"Forecasttidspunkt\",inplace=True)\n",
        "\n",
        "  #Dropping non relevant columns\n",
        "  df_ml.rename(columns={\"CUSIP\":\"cusip\"},inplace=True)\n",
        "  list_drop = [\"SHROUT\",\"company_name\",\"cusip\",\"analyst_EPS_mean\",\"analyst_high\",\"analyst_low\",\"analyst_std_mean\",\"periode_regnskabstal\",\"PERMNO\",\"date\",\"slutmåned\",\"Instrument\"]\n",
        "  df_ml.drop(list_drop,axis=1,inplace=True,errors=\"ignore\")\n",
        "\n",
        "  # Creating year and month\n",
        "  df_ml[\"month\"] = df_ml.index.month\n",
        "  df_ml[\"year\"] = df_ml.index.year\n",
        "  df_ml = df_ml.astype({'GVKEY': 'category'})\n",
        "  df_ml = df_ml.astype({'industry_fama': 'category'})\n",
        "  df_ml.sort_index(inplace=True)\n",
        "  \n",
        "  # New features og duplikat kolonner\n",
        "  df_ml[\"volume_usd\"] = df_ml[\"ALTPRC\"] * df_ml[\"VOL\"]\n",
        "  df_ml[\"Div_yield\"] = df_ml[\"DPS_ex_date(dvpsxq)\"] / df_ml[\"ALTPRC\"]\n",
        "  df_ml.drop([\"PRC\",\"adj_close\"],axis=1,inplace=True)\n",
        "\n",
        "  columns_drop_list = [\"Assets_total(Atq)\",\"industry_return\",\"volume_usd\",\"VOL\"]\n",
        "  df_ml.drop(columns_drop_list,axis=1,inplace=True)\n",
        "\n",
        "  \n",
        "  # Lagging variable\n",
        "\n",
        "  columns_lag = ['EPS_actual', 'Accounts_payable(Apq)',\n",
        "       'Assets_total(Atq)', 'COGS(Cogsq)',\n",
        "       'Common_equity(Ceqq)', 'DPS_ex_date(dvpsxq)', 'Deprect_and_ammor(Dpq)',\n",
        "       'EPS_lagged', 'GVKEY', 'Income taxes_total(Txtq)',\n",
        "       'Non_operating_income(Nopiq)', 'Operating_income_after_deprec(Oiadpq)',\n",
        "       'Opex_total(Xoprq)', 'PP&E_total_net(Ppentq)','PRC',\n",
        "       'Pretax_income(Piq)', 'Receviables(Rectq)', 'Revenue_total(Revtq)',\n",
        "    'VOL', 'adj_return','adj_close','market_cap', 'niq', 'industry_return', 'merafkast', 'ALTPRC','volume_usd','Div_yield']\n",
        "\n",
        "  df_lag = df_ml.copy()\n",
        "  df_lag = df_lag[df_lag.columns.intersection(columns_lag)]\n",
        "  df_lag_new = df_lag.copy()\n",
        "\n",
        "  for column in df_lag.columns:\n",
        "    for i in range(0,1):\n",
        "      df_lag_new[f'{column}_{i}'] = df_lag.groupby(\"GVKEY\")[f'{column}'].shift(i).values\n",
        "\n",
        "  columns_to_choose = ['Assets_total(Atq)','Accounts_payable(Apq)', 'COGS(Cogsq)',\n",
        "       'Common_equity(Ceqq)', 'DPS_ex_date(dvpsxq)', 'Deprect_and_ammor(Dpq)',\n",
        "       'EPS_lagged', 'Income taxes_total(Txtq)',\n",
        "       'Non_operating_income(Nopiq)', 'Operating_income_after_deprec(Oiadpq)',\n",
        "       'Opex_total(Xoprq)', 'PP&E_total_net(Ppentq)',\n",
        "       'Pretax_income(Piq)', 'Receviables(Rectq)', 'Revenue_total(Revtq)',\n",
        "       'market_cap', 'niq',\"Div_yield\",\"ALTPRC\",'adj_return',\"industry_return\",\"merafkast\",'VOL','volume_usd','PRC','adj_close']\n",
        "  col_lag_choose = []\n",
        "  for element in columns_to_choose:\n",
        "    for i in range(0,1):\n",
        "      col_lag_choose.append(f'{element}_{i}')\n",
        "  col_lag_choose.append(\"GVKEY\")\n",
        "\n",
        "  # Vælger kun de endelige features\n",
        "  df_lag_new = df_lag_new[df_lag_new.columns.intersection(col_lag_choose)]\n",
        "\n",
        "  # Merge lags og oprindelig dataframe sammen\n",
        "  df_ml = df_ml.reset_index().merge(df_lag_new.reset_index(),how=\"inner\",on=[\"GVKEY\",\"Forecasttidspunkt\"])\n",
        "  \n",
        "  # Fikse indeks og erstatte NA i lags med NA\n",
        "\n",
        "  numeric_columns = df_ml.select_dtypes(include=['number']).columns\n",
        "  df_ml[numeric_columns] = df_ml[numeric_columns].fillna(0)\n",
        "\n",
        "  df_ml.set_index(\"Forecasttidspunkt\",inplace=True)\n",
        "  \n",
        "  cutoff_date = \"2016-06-30\"\n",
        "  validation_start = \"2016-09-30\"\n",
        "  validation_end = \"2018-06-30\"\n",
        "  test_start = \"2018-09-30\"\n",
        "\n",
        "  training_df = df_ml.loc[:cutoff_date,:]\n",
        "  validation_df = df_ml.loc[validation_start:validation_end,:]\n",
        "  test_df = df_ml.loc[test_start:,:]\n",
        "\n",
        "  training_df_cv =  df_ml.loc[:validation_end,:]\n",
        "\n",
        "# X og Y\n",
        "\n",
        "  training_y = training_df[\"EPS_actual\"]\n",
        "  validatation_y = validation_df[\"EPS_actual\"]\n",
        "  test_y = test_df[\"EPS_actual\"]\n",
        "  training_y_cv = training_df_cv[\"EPS_actual\"]\n",
        "\n",
        "  training_x = training_df.drop(\"EPS_actual\",axis=1)\n",
        "  training_x_cv = training_df_cv.drop(\"EPS_actual\",axis=1)\n",
        "  validatation_x = validation_df.drop(\"EPS_actual\",axis=1)\n",
        "  test_x = test_df.drop(\"EPS_actual\",axis=1)\n",
        "\n",
        "\n",
        "  from catboost import CatBoostRegressor\n",
        "  from catboost import Pool\n",
        "\n",
        "  cat_model = CatBoostRegressor(cat_features=[\"GVKEY\",\"industry_fama\"],random_seed=2021,loss_function=\"MAE\",early_stopping_rounds=250,iterations=50,max_ctr_complexity=0,one_hot_max_size=5000)\n",
        "\n",
        "  pool_val = Pool(validatation_x,validatation_y,cat_features=[\"GVKEY\",\"industry_fama\"])\n",
        "\n",
        "  cat_model.fit(training_x,training_y,eval_set=pool_val,verbose=False)\n",
        "\n",
        "  print(cat_model.best_iteration_)\n",
        "  print(cat_model.get_best_score())\n",
        "\n",
        "  test_pred = cat_model.predict(test_x)\n",
        "  return training_df_cv, training_x_cv,training_y_cv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621083754726
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "fil_sti = \"input.csv.zip\"\n",
        "\n",
        "training_df_cv, training_x_cv,training_y_cv = ml_pipeline(fil_sti)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621083756564
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pd.Series(training_df_cv.columns).to_excel(\"excel_features.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621083931530
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "(training_df_cv[\"EPS_lagged\"] == training_df_cv[\"EPS_lagged_0\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1621083842257
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "training_df_cv[[\"EPS_lagged\",\"EPS_lagged_0\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross Validating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1617297010219
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import TimeSeriesSplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1617297011317
        }
      },
      "outputs": [],
      "source": [
        "ts_cv = TimeSeriesSplit(n_splits=5,test_size=10000)\n",
        "\n",
        "for train_index, test_index in ts_cv.split(training_df_cv):\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1617300244734
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "paramsxxx = {\"cat_features\": [\"GVKEY\",\"industry_fama\"], \"random_seed\": 2021, \"loss_function\": \"MAE\", \"max_ctr_complexity\" : 0,\"one_hot_max_size\":0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1617299056098
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from catboost import cv,Pool\n",
        "pool_data = Pool(data=training_x_cv,label=training_y_cv, cat_features = [\"GVKEY\",\"industry_fama\"])\n",
        "\n",
        "\n",
        "pandas_score = cv(pool=pool_data,params=params, iterations=50, early_stopping_rounds=3,folds=ts_cv,verbose=False,as_pandas=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1617299021190
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optuna Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1617305444603
        }
      },
      "outputs": [],
      "source": [
        "from optuna.integration.tensorboard import TensorBoardCallback\n",
        "\n",
        "from azureml.tensorboard import Tensorboard\n",
        "\n",
        "tb = Tensorboard([],local_root=\"logs/\")\n",
        "\n",
        "# If successful, start() returns a string with the URI of the instance.\n",
        "tb.stop()\n",
        "tb.start()\n",
        "\n",
        "# After your job completes, be sure to stop() the streaming otherwise it will continue to run. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1617301820406
        }
      },
      "outputs": [],
      "source": [
        "! pip install --quiet optuna\n",
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "from optuna.samplers import TPESampler\n",
        "sampler = TPESampler(seed=2021)\n",
        "pool_data = Pool(data=training_x_cv,label=training_y_cv, cat_features = [\"GVKEY\",\"industry_fama\"])\n",
        "# defining the trial object with parameters and metric that needs to maximized \n",
        "def objective(trial):\n",
        "\n",
        "    param = {\n",
        "        \"random_state\": 2021,\n",
        "        \"verbose\": False,\n",
        "        \"loss_function\": \"MAE\",\n",
        "        \"max_ctr_complexity\": 0,\n",
        "        \"one_hot_max_size\": 5000,\n",
        "        \"cat_features\": [\"GVKEY\",\"industry_fama\"],\n",
        "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
        "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
        "        \"bootstrap_type\": trial.suggest_categorical(\n",
        "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
        "        )\n",
        "    }\n",
        "\n",
        "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
        "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
        "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
        "\n",
        "    \n",
        "# creating the crossval score. This outputs the mean of the five cross validations runs. \n",
        "    cross_val_scores = cv(pool=pool_data,params=param, iterations=1000, early_stopping_rounds=250,folds=ts_cv,verbose=False,as_pandas=False)[\"test-MAE-mean\"][-1]\n",
        "\n",
        "# returns the cross val score so the subsequnt optimizere know what to maximize\n",
        "    return cross_val_scores\n",
        "\n",
        "tensorboard_callback = TensorBoardCallback(\"logs/\", metric_name=\"MAE\")\n",
        "# Defining the optuna optimizer\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"minimize\",sampler=sampler)\n",
        "    study.optimize(objective,timeout=43200,callbacks=[tensorboard_callback])\n",
        "\n",
        "# Some formatting of the output\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "\n",
        "# Saving the Study element to a pickle file\n",
        "\n",
        "import joblib\n",
        "joblib.dump(study,\"hyperparametertuning_study.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1617301897212
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}